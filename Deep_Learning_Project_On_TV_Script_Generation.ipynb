{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Project On TV Script Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/divyalalithaprudhivi/Assignment/blob/master/Deep_Learning_Project_On_TV_Script_Generation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vwg7FBIcV11_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load Dataset from File\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
        "    \"\"\"\n",
        "    Preprocess Text Data.  Save to to file.\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    source_text = load_data(source_path)\n",
        "    target_text = load_data(target_path)\n",
        "\n",
        "    source_text = source_text.lower()\n",
        "    target_text = target_text.lower()\n",
        "\n",
        "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
        "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
        "\n",
        "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
        "\n",
        "    # Save Data\n",
        "    pickle.dump((\n",
        "        (source_text, target_text),\n",
        "        (source_vocab_to_int, target_vocab_to_int),\n",
        "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_preprocess():\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    return pickle.load(open('preprocess.p', mode='rb'))\n",
        "\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    \"\"\"\n",
        "    vocab = set(text.split())\n",
        "    vocab_to_int = copy.copy(CODES)\n",
        "\n",
        "    for v_i, v in enumerate(vocab, len(CODES)):\n",
        "        vocab_to_int[v] = v_i\n",
        "\n",
        "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab\n",
        "\n",
        "\n",
        "def save_params(params):\n",
        "    \"\"\"\n",
        "    Save parameters to file\n",
        "    \"\"\"\n",
        "    pickle.dump(params, open('params.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "    return pickle.load(open('params.p', mode='rb'))\n",
        "\n",
        "\n",
        "def batch_data(source, target, batch_size):\n",
        "    \"\"\"\n",
        "    Batch source and target together\n",
        "    \"\"\"\n",
        "    for batch_i in range(0, len(source)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        source_batch = source[start_i:start_i + batch_size]\n",
        "        target_batch = target[start_i:start_i + batch_size]\n",
        "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
        "\n",
        "\n",
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"\n",
        "    Pad sentence with <PAD> id\n",
        "    \"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [CODES['<PAD>']] * (max_sentence - len(sentence))\n",
        "            for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ofkkhr5z_Vp2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dnzbHg_v_VqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "ee092575-0fdb-46e8-f855-c159a7570501"
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "source_path = 'small_vocab_en.txt'\n",
        "target_path = 'small_vocab_fr.txt'\n",
        "source_text = load_data(source_path)\n",
        "target_text = load_data(target_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9dd419d2f936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'small_vocab_en.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'small_vocab_fr.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msource_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtarget_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-dafe424762fc>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'small_vocab_en.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "94CaIaF5_Vq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "a8d38b67-1197-42cf-e5b2-2b221a5ca3ed"
      },
      "cell_type": "code",
      "source": [
        "view_sentence_range = (0, 10)\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
        "\n",
        "sentences = source_text.split('\\n')\n",
        "word_counts = [len(sentence.split()) for sentence in sentences]\n",
        "print('Number of sentences: {}'.format(len(sentences)))\n",
        "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
        "\n",
        "print()\n",
        "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
        "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
        "print()\n",
        "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
        "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Stats\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e138b43d157d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset Stats'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Roughly the number of unique words: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'source_text' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "l-kydeV1_Vrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "    Convert source and target text to proper word ids\n",
        "    :param source_text: String that contains all the source text.\n",
        "    :param target_text: String that contains all the target text.\n",
        "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
        "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
        "    :return: A tuple of lists (source_id_text, target_id_text)\n",
        "    \"\"\"\n",
        "    source_sentences = source_text.split('\\n')\n",
        "    target_sentences = [sentence + ' <EOS>' for sentence in target_text.split('\\n')]\n",
        "    source_ids = list(map(lambda x: [source_vocab_to_int[word] for word in x.split()], source_sentences))\n",
        "    target_ids = list(map(lambda x: [target_vocab_to_int[word] for word in x.split()], target_sentences))\n",
        "    return source_ids, target_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwHLKsnQ_Vrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocess_and_save_data(source_path, target_path, text_to_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LC4nrxTs_VsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PoEWbEZ__Vss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_inputs():\n",
        "    \"\"\"\n",
        "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
        "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
        "    max target sequence length, source sequence length)\n",
        "    \"\"\"\n",
        "    \n",
        "    input = tf.placeholder(tf.int32,shape=[None,None],name='input')\n",
        "    targets = tf.placeholder(tf.int32,shape=[None,None],name='targets')\n",
        "    learningrate = tf.placeholder(tf.float32,shape=[],name='learningrate')\n",
        "    keep_prob = tf.placeholder(tf.float32,shape=[],name='keep_prob')\n",
        "    target_sequence_length = tf.placeholder(tf.int32,[None,],name='target_sequence_length')\n",
        "    max_target_length = tf.reduce_max(target_sequence_length)\n",
        "    source_sequence_length = tf.placeholder(tf.int32,[None,],name='source_sequence_length')\n",
        "    \n",
        "    return (input,targets,learningrate,keep_prob,target_sequence_length,max_target_length,source_sequence_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B--50RG8_VtH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
        "    \"\"\"\n",
        "    Preprocess target data for encoding\n",
        "    :param target_data: Target Placehoder\n",
        "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
        "    :param batch_size: Batch Size\n",
        "    :return: Preprocessed target data\n",
        "    \"\"\"\n",
        "    ending = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1,1])\n",
        "    decoder_input = tf.concat([tf.fill([batch_size,1], target_vocab_to_int['<GO>']), ending], 1)\n",
        "    return decoder_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tT5_5s3__Vt9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
        "                   source_sequence_length, source_vocab_size, \n",
        "                   encoding_embedding_size):\n",
        "    \"\"\"\n",
        "    Create encoding layer\n",
        "    :param rnn_inputs: Inputs for the RNN\n",
        "    :param rnn_size: RNN Size\n",
        "    :param num_layers: Number of layers\n",
        "    :param keep_prob: Dropout keep probability\n",
        "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
        "    :param source_vocab_size: vocabulary size of source data\n",
        "    :param encoding_embedding_size: embedding size of source data\n",
        "    :return: tuple (RNN output, RNN state)\n",
        "    \"\"\"\n",
        "    # Encoder embedding\n",
        "    enc_embed = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size, encoding_embedding_size)\n",
        "    \n",
        "    def create_cell(rnn_size):\n",
        "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
        "        drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "    \n",
        "    enc_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
        "    encoding_output, encoding_state = tf.nn.dynamic_rnn(enc_cell, enc_embed, \n",
        "                                                        sequence_length=source_sequence_length,dtype=tf.float32)\n",
        "    \n",
        "    return encoding_output, encoding_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jUHTSjBN_Vup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
        "                         target_sequence_length, max_summary_length, \n",
        "                         output_layer, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a decoding layer for training\n",
        "    :param encoder_state: Encoder State\n",
        "    :param dec_cell: Decoder RNN Cell\n",
        "    :param dec_embed_input: Decoder embedded input\n",
        "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
        "    :param max_summary_length: The length of the longest sequence in the batch\n",
        "    :param output_layer: Function to apply the output layer\n",
        "    :param keep_prob: Dropout keep probability\n",
        "    :return: BasicDecoderOutput containing training logits and sample_id\n",
        "    \"\"\"\n",
        "\n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=target_sequence_length,\n",
        "                                                        time_major=False)\n",
        "    \n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)\n",
        "    \n",
        "    training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                                impute_finished=True,\n",
        "                                                                maximum_iterations=max_summary_length)[0]\n",
        "    return training_decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0EwXRZXh_VvE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
        "                         end_of_sequence_id, max_target_sequence_length,\n",
        "                         vocab_size, output_layer, batch_size, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a decoding layer for inference\n",
        "    :param encoder_state: Encoder state\n",
        "    :param dec_cell: Decoder RNN Cell\n",
        "    :param dec_embeddings: Decoder embeddings\n",
        "    :param start_of_sequence_id: GO ID\n",
        "    :param end_of_sequence_id: EOS Id\n",
        "    :param max_target_sequence_length: Maximum length of target sequences\n",
        "    :param vocab_size: Size of decoder/target vocabulary\n",
        "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
        "    :param output_layer: Function to apply the output layer\n",
        "    :param batch_size: Batch size\n",
        "    :param keep_prob: Dropout keep probability\n",
        "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
        "    \"\"\"\n",
        "\n",
        "    start_tokens = tf.tile(tf.constant([start_of_sequence_id], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_of_sequence_id)\n",
        "    \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        encoder_state,\n",
        "                                                        output_layer)\n",
        "    \n",
        "    inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
        "    return inference_decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4G_2kVGC_Vvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_input, encoder_state,\n",
        "                   target_sequence_length, max_target_sequence_length,\n",
        "                   rnn_size,\n",
        "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, decoding_embedding_size):\n",
        "    \"\"\"\n",
        "    Create decoding layer\n",
        "    :param dec_input: Decoder input\n",
        "    :param encoder_state: Encoder state\n",
        "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
        "    :param max_target_sequence_length: Maximum length of target sequences\n",
        "    :param rnn_size: RNN Size\n",
        "    :param num_layers: Number of layers\n",
        "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
        "    :param target_vocab_size: Size of target vocabulary\n",
        "    :param batch_size: The size of the batch\n",
        "    :param keep_prob: Dropout keep probability\n",
        "    :param decoding_embedding_size: Decoding embedding size\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    # 1. Decoder Embedding\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    # 2. Construct the decoder cell\n",
        "    def create_cell(rnn_size):\n",
        "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                            initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
        "        drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "\n",
        "    dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
        "    \n",
        "    output_layer = Dense(target_vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        train_decoder_out = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
        "                         target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n",
        "        \n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        infer_decoder_out = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, \n",
        "                             target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, \n",
        "                             target_vocab_size, output_layer, batch_size, keep_prob)\n",
        "        \n",
        "    return (train_decoder_out, infer_decoder_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iQttiyk_Vv3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
        "                  source_sequence_length, target_sequence_length,\n",
        "                  max_target_sentence_length,\n",
        "                  source_vocab_size, target_vocab_size,\n",
        "                  enc_embedding_size, dec_embedding_size,\n",
        "                  rnn_size, num_layers, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "    Build the Sequence-to-Sequence part of the neural network\n",
        "    :param input_data: Input placeholder\n",
        "    :param target_data: Target placeholder\n",
        "    :param keep_prob: Dropout keep probability placeholder\n",
        "    :param batch_size: Batch Size\n",
        "    :param source_sequence_length: Sequence Lengths of source sequences in the batch\n",
        "    :param target_sequence_length: Sequence Lengths of target sequences in the batch\n",
        "    :param source_vocab_size: Source vocabulary size\n",
        "    :param target_vocab_size: Target vocabulary size\n",
        "    :param enc_embedding_size: Decoder embedding size\n",
        "    :param dec_embedding_size: Encoder embedding size\n",
        "    :param rnn_size: RNN Size\n",
        "    :param num_layers: Number of layers\n",
        "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    _, enc_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, \n",
        "                   source_sequence_length, source_vocab_size, \n",
        "                   enc_embedding_size)\n",
        "    \n",
        "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
        "    \n",
        "    training_decoder_output, inference_decoder_output = decoding_layer(dec_input, enc_state,\n",
        "                   target_sequence_length, max_target_sentence_length,\n",
        "                   rnn_size,\n",
        "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, dec_embedding_size)\n",
        "    \n",
        "    return training_decoder_output, inference_decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Moat4Y6N_VwF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of Epochs\n",
        "epochs = 10\n",
        "# Batch Size\n",
        "batch_size = 256\n",
        "# RNN Size\n",
        "rnn_size = 512\n",
        "# Number of Layers\n",
        "num_layers = 2\n",
        "# Embedding Size\n",
        "encoding_embedding_size = 256\n",
        "decoding_embedding_size = 256\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Dropout Keep Probability\n",
        "keep_probability = 0.75\n",
        "display_step = 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYNPQRXW_VwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.layers.core import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_JGjeo9o_Vws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_path = 'checkpoints/dev'\n",
        "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
        "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
        "\n",
        "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
        "    input_shape = tf.shape(input_data)\n",
        "\n",
        "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                   targets,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size,\n",
        "                                                   source_sequence_length,\n",
        "                                                   target_sequence_length,\n",
        "                                                   max_target_sequence_length,\n",
        "                                                   len(source_vocab_to_int),\n",
        "                                                   len(target_vocab_to_int),\n",
        "                                                   encoding_embedding_size,\n",
        "                                                   decoding_embedding_size,\n",
        "                                                   rnn_size,\n",
        "                                                   num_layers,\n",
        "                                                   target_vocab_to_int)\n",
        "\n",
        "\n",
        "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "\n",
        "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FeXZ7v3_Vw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(sources)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:start_i + batch_size]\n",
        "        targets_batch = targets[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
        "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in pad_targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in pad_sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhv5uSOTjcGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3KUyHSB9_Vxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9044
        },
        "outputId": "1a4be900-c9f2-4840-ab95-f07c334cbff1"
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(target, logits):\n",
        "    \"\"\"\n",
        "    Calculate accuracy\n",
        "    \"\"\"\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0,0),(0,max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0,0),(0,max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))\n",
        "\n",
        "# Split data to training and validation sets\n",
        "train_source = source_int_text[batch_size:]\n",
        "train_target = target_int_text[batch_size:]\n",
        "valid_source = source_int_text[:batch_size]\n",
        "valid_target = target_int_text[:batch_size]\n",
        "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
        "                                                                                                             valid_target,\n",
        "                                                                                                             batch_size,\n",
        "                                                                                                             source_vocab_to_int['<PAD>'],\n",
        "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
        "                get_batches(train_source, train_target, batch_size,\n",
        "                            source_vocab_to_int['<PAD>'],\n",
        "                            target_vocab_to_int['<PAD>'])):\n",
        "\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: source_batch,\n",
        "                 targets: target_batch,\n",
        "                 lr: learning_rate,\n",
        "                 target_sequence_length: targets_lengths,\n",
        "                 source_sequence_length: sources_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "\n",
        "\n",
        "                batch_train_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: source_batch,\n",
        "                     source_sequence_length: sources_lengths,\n",
        "                     target_sequence_length: targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "\n",
        "                batch_valid_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: valid_sources_batch,\n",
        "                     source_sequence_length: valid_sources_lengths,\n",
        "                     target_sequence_length: valid_targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
        "\n",
        "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
        "\n",
        "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
        "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch   10/538 - Train Accuracy: 0.3355, Validation Accuracy: 0.4164, Loss: 3.2124\n",
            "Epoch   0 Batch   20/538 - Train Accuracy: 0.4347, Validation Accuracy: 0.4723, Loss: 2.5069\n",
            "Epoch   0 Batch   30/538 - Train Accuracy: 0.4395, Validation Accuracy: 0.5062, Loss: 2.2804\n",
            "Epoch   0 Batch   40/538 - Train Accuracy: 0.4867, Validation Accuracy: 0.4849, Loss: 1.7755\n",
            "Epoch   0 Batch   50/538 - Train Accuracy: 0.4139, Validation Accuracy: 0.4785, Loss: 1.7539\n",
            "Epoch   0 Batch   60/538 - Train Accuracy: 0.4260, Validation Accuracy: 0.4819, Loss: 1.5923\n",
            "Epoch   0 Batch   70/538 - Train Accuracy: 0.4786, Validation Accuracy: 0.5071, Loss: 1.4185\n",
            "Epoch   0 Batch   80/538 - Train Accuracy: 0.4564, Validation Accuracy: 0.4943, Loss: 1.3933\n",
            "Epoch   0 Batch   90/538 - Train Accuracy: 0.5171, Validation Accuracy: 0.5332, Loss: 1.2263\n",
            "Epoch   0 Batch  100/538 - Train Accuracy: 0.4719, Validation Accuracy: 0.5048, Loss: 1.1462\n",
            "Epoch   0 Batch  110/538 - Train Accuracy: 0.5021, Validation Accuracy: 0.5384, Loss: 1.0904\n",
            "Epoch   0 Batch  120/538 - Train Accuracy: 0.5400, Validation Accuracy: 0.5630, Loss: 0.9502\n",
            "Epoch   0 Batch  130/538 - Train Accuracy: 0.5658, Validation Accuracy: 0.5662, Loss: 0.8764\n",
            "Epoch   0 Batch  140/538 - Train Accuracy: 0.5504, Validation Accuracy: 0.5763, Loss: 0.9222\n",
            "Epoch   0 Batch  150/538 - Train Accuracy: 0.5949, Validation Accuracy: 0.5909, Loss: 0.8292\n",
            "Epoch   0 Batch  160/538 - Train Accuracy: 0.6036, Validation Accuracy: 0.5902, Loss: 0.7590\n",
            "Epoch   0 Batch  170/538 - Train Accuracy: 0.6202, Validation Accuracy: 0.6058, Loss: 0.7306\n",
            "Epoch   0 Batch  180/538 - Train Accuracy: 0.6404, Validation Accuracy: 0.6204, Loss: 0.7064\n",
            "Epoch   0 Batch  190/538 - Train Accuracy: 0.6138, Validation Accuracy: 0.6225, Loss: 0.6985\n",
            "Epoch   0 Batch  200/538 - Train Accuracy: 0.6312, Validation Accuracy: 0.6291, Loss: 0.6591\n",
            "Epoch   0 Batch  210/538 - Train Accuracy: 0.6062, Validation Accuracy: 0.6206, Loss: 0.6360\n",
            "Epoch   0 Batch  220/538 - Train Accuracy: 0.6092, Validation Accuracy: 0.6250, Loss: 0.5996\n",
            "Epoch   0 Batch  230/538 - Train Accuracy: 0.6402, Validation Accuracy: 0.6465, Loss: 0.6002\n",
            "Epoch   0 Batch  240/538 - Train Accuracy: 0.6404, Validation Accuracy: 0.6442, Loss: 0.6020\n",
            "Epoch   0 Batch  250/538 - Train Accuracy: 0.6521, Validation Accuracy: 0.6614, Loss: 0.5591\n",
            "Epoch   0 Batch  260/538 - Train Accuracy: 0.6628, Validation Accuracy: 0.6422, Loss: 0.5347\n",
            "Epoch   0 Batch  270/538 - Train Accuracy: 0.6443, Validation Accuracy: 0.6584, Loss: 0.5302\n",
            "Epoch   0 Batch  280/538 - Train Accuracy: 0.7219, Validation Accuracy: 0.6916, Loss: 0.4801\n",
            "Epoch   0 Batch  290/538 - Train Accuracy: 0.6787, Validation Accuracy: 0.6946, Loss: 0.4724\n",
            "Epoch   0 Batch  300/538 - Train Accuracy: 0.7245, Validation Accuracy: 0.7175, Loss: 0.4545\n",
            "Epoch   0 Batch  310/538 - Train Accuracy: 0.7516, Validation Accuracy: 0.7255, Loss: 0.4451\n",
            "Epoch   0 Batch  320/538 - Train Accuracy: 0.7437, Validation Accuracy: 0.7516, Loss: 0.3964\n",
            "Epoch   0 Batch  330/538 - Train Accuracy: 0.7766, Validation Accuracy: 0.7436, Loss: 0.3534\n",
            "Epoch   0 Batch  340/538 - Train Accuracy: 0.7889, Validation Accuracy: 0.7702, Loss: 0.3649\n",
            "Epoch   0 Batch  350/538 - Train Accuracy: 0.7721, Validation Accuracy: 0.7924, Loss: 0.3471\n",
            "Epoch   0 Batch  360/538 - Train Accuracy: 0.8012, Validation Accuracy: 0.7891, Loss: 0.3185\n",
            "Epoch   0 Batch  370/538 - Train Accuracy: 0.8051, Validation Accuracy: 0.8020, Loss: 0.3041\n",
            "Epoch   0 Batch  380/538 - Train Accuracy: 0.8336, Validation Accuracy: 0.8049, Loss: 0.2612\n",
            "Epoch   0 Batch  390/538 - Train Accuracy: 0.8531, Validation Accuracy: 0.8306, Loss: 0.2321\n",
            "Epoch   0 Batch  400/538 - Train Accuracy: 0.8372, Validation Accuracy: 0.8255, Loss: 0.2379\n",
            "Epoch   0 Batch  410/538 - Train Accuracy: 0.8523, Validation Accuracy: 0.8503, Loss: 0.2300\n",
            "Epoch   0 Batch  420/538 - Train Accuracy: 0.8918, Validation Accuracy: 0.8450, Loss: 0.1976\n",
            "Epoch   0 Batch  430/538 - Train Accuracy: 0.8719, Validation Accuracy: 0.8436, Loss: 0.1827\n",
            "Epoch   0 Batch  440/538 - Train Accuracy: 0.8623, Validation Accuracy: 0.8482, Loss: 0.1986\n",
            "Epoch   0 Batch  450/538 - Train Accuracy: 0.8778, Validation Accuracy: 0.8832, Loss: 0.1781\n",
            "Epoch   0 Batch  460/538 - Train Accuracy: 0.8743, Validation Accuracy: 0.8672, Loss: 0.1671\n",
            "Epoch   0 Batch  470/538 - Train Accuracy: 0.8830, Validation Accuracy: 0.8556, Loss: 0.1396\n",
            "Epoch   0 Batch  480/538 - Train Accuracy: 0.9062, Validation Accuracy: 0.8869, Loss: 0.1356\n",
            "Epoch   0 Batch  490/538 - Train Accuracy: 0.8986, Validation Accuracy: 0.8984, Loss: 0.1249\n",
            "Epoch   0 Batch  500/538 - Train Accuracy: 0.9306, Validation Accuracy: 0.8773, Loss: 0.1077\n",
            "Epoch   0 Batch  510/538 - Train Accuracy: 0.9226, Validation Accuracy: 0.8977, Loss: 0.1154\n",
            "Epoch   0 Batch  520/538 - Train Accuracy: 0.9184, Validation Accuracy: 0.8908, Loss: 0.1228\n",
            "Epoch   0 Batch  530/538 - Train Accuracy: 0.8908, Validation Accuracy: 0.9031, Loss: 0.1219\n",
            "Epoch   1 Batch   10/538 - Train Accuracy: 0.9121, Validation Accuracy: 0.9022, Loss: 0.1036\n",
            "Epoch   1 Batch   20/538 - Train Accuracy: 0.9217, Validation Accuracy: 0.9096, Loss: 0.0920\n",
            "Epoch   1 Batch   30/538 - Train Accuracy: 0.9152, Validation Accuracy: 0.8981, Loss: 0.1056\n",
            "Epoch   1 Batch   40/538 - Train Accuracy: 0.9201, Validation Accuracy: 0.8995, Loss: 0.0703\n",
            "Epoch   1 Batch   50/538 - Train Accuracy: 0.9100, Validation Accuracy: 0.9018, Loss: 0.0924\n",
            "Epoch   1 Batch   60/538 - Train Accuracy: 0.9289, Validation Accuracy: 0.9118, Loss: 0.0792\n",
            "Epoch   1 Batch   70/538 - Train Accuracy: 0.9128, Validation Accuracy: 0.9110, Loss: 0.0815\n",
            "Epoch   1 Batch   80/538 - Train Accuracy: 0.9260, Validation Accuracy: 0.9187, Loss: 0.0884\n",
            "Epoch   1 Batch   90/538 - Train Accuracy: 0.9310, Validation Accuracy: 0.9135, Loss: 0.0866\n",
            "Epoch   1 Batch  100/538 - Train Accuracy: 0.9234, Validation Accuracy: 0.9199, Loss: 0.0691\n",
            "Epoch   1 Batch  110/538 - Train Accuracy: 0.9301, Validation Accuracy: 0.9263, Loss: 0.0746\n",
            "Epoch   1 Batch  120/538 - Train Accuracy: 0.9504, Validation Accuracy: 0.9180, Loss: 0.0575\n",
            "Epoch   1 Batch  130/538 - Train Accuracy: 0.9267, Validation Accuracy: 0.9350, Loss: 0.0646\n",
            "Epoch   1 Batch  140/538 - Train Accuracy: 0.9098, Validation Accuracy: 0.9350, Loss: 0.0821\n",
            "Epoch   1 Batch  150/538 - Train Accuracy: 0.9309, Validation Accuracy: 0.9329, Loss: 0.0624\n",
            "Epoch   1 Batch  160/538 - Train Accuracy: 0.9185, Validation Accuracy: 0.9274, Loss: 0.0591\n",
            "Epoch   1 Batch  170/538 - Train Accuracy: 0.9176, Validation Accuracy: 0.9371, Loss: 0.0696\n",
            "Epoch   1 Batch  180/538 - Train Accuracy: 0.9412, Validation Accuracy: 0.9318, Loss: 0.0672\n",
            "Epoch   1 Batch  190/538 - Train Accuracy: 0.9204, Validation Accuracy: 0.9197, Loss: 0.0816\n",
            "Epoch   1 Batch  200/538 - Train Accuracy: 0.9482, Validation Accuracy: 0.9375, Loss: 0.0541\n",
            "Epoch   1 Batch  210/538 - Train Accuracy: 0.9221, Validation Accuracy: 0.9492, Loss: 0.0630\n",
            "Epoch   1 Batch  220/538 - Train Accuracy: 0.9200, Validation Accuracy: 0.9508, Loss: 0.0637\n",
            "Epoch   1 Batch  230/538 - Train Accuracy: 0.9391, Validation Accuracy: 0.9418, Loss: 0.0535\n",
            "Epoch   1 Batch  240/538 - Train Accuracy: 0.9373, Validation Accuracy: 0.9460, Loss: 0.0631\n",
            "Epoch   1 Batch  250/538 - Train Accuracy: 0.9457, Validation Accuracy: 0.9345, Loss: 0.0569\n",
            "Epoch   1 Batch  260/538 - Train Accuracy: 0.9094, Validation Accuracy: 0.9462, Loss: 0.0576\n",
            "Epoch   1 Batch  270/538 - Train Accuracy: 0.9383, Validation Accuracy: 0.9350, Loss: 0.0476\n",
            "Epoch   1 Batch  280/538 - Train Accuracy: 0.9581, Validation Accuracy: 0.9412, Loss: 0.0471\n",
            "Epoch   1 Batch  290/538 - Train Accuracy: 0.9668, Validation Accuracy: 0.9329, Loss: 0.0463\n",
            "Epoch   1 Batch  300/538 - Train Accuracy: 0.9232, Validation Accuracy: 0.9478, Loss: 0.0594\n",
            "Epoch   1 Batch  310/538 - Train Accuracy: 0.9609, Validation Accuracy: 0.9540, Loss: 0.0559\n",
            "Epoch   1 Batch  320/538 - Train Accuracy: 0.9520, Validation Accuracy: 0.9450, Loss: 0.0492\n",
            "Epoch   1 Batch  330/538 - Train Accuracy: 0.9550, Validation Accuracy: 0.9379, Loss: 0.0483\n",
            "Epoch   1 Batch  340/538 - Train Accuracy: 0.9471, Validation Accuracy: 0.9556, Loss: 0.0482\n",
            "Epoch   1 Batch  350/538 - Train Accuracy: 0.9483, Validation Accuracy: 0.9446, Loss: 0.0525\n",
            "Epoch   1 Batch  360/538 - Train Accuracy: 0.9500, Validation Accuracy: 0.9600, Loss: 0.0434\n",
            "Epoch   1 Batch  370/538 - Train Accuracy: 0.9502, Validation Accuracy: 0.9609, Loss: 0.0469\n",
            "Epoch   1 Batch  380/538 - Train Accuracy: 0.9625, Validation Accuracy: 0.9508, Loss: 0.0396\n",
            "Epoch   1 Batch  390/538 - Train Accuracy: 0.9323, Validation Accuracy: 0.9503, Loss: 0.0383\n",
            "Epoch   1 Batch  400/538 - Train Accuracy: 0.9522, Validation Accuracy: 0.9609, Loss: 0.0463\n",
            "Epoch   1 Batch  410/538 - Train Accuracy: 0.9604, Validation Accuracy: 0.9480, Loss: 0.0480\n",
            "Epoch   1 Batch  420/538 - Train Accuracy: 0.9600, Validation Accuracy: 0.9451, Loss: 0.0457\n",
            "Epoch   1 Batch  430/538 - Train Accuracy: 0.9393, Validation Accuracy: 0.9446, Loss: 0.0418\n",
            "Epoch   1 Batch  440/538 - Train Accuracy: 0.9553, Validation Accuracy: 0.9556, Loss: 0.0451\n",
            "Epoch   1 Batch  450/538 - Train Accuracy: 0.9397, Validation Accuracy: 0.9522, Loss: 0.0580\n",
            "Epoch   1 Batch  460/538 - Train Accuracy: 0.9492, Validation Accuracy: 0.9409, Loss: 0.0445\n",
            "Epoch   1 Batch  470/538 - Train Accuracy: 0.9613, Validation Accuracy: 0.9407, Loss: 0.0416\n",
            "Epoch   1 Batch  480/538 - Train Accuracy: 0.9608, Validation Accuracy: 0.9563, Loss: 0.0379\n",
            "Epoch   1 Batch  490/538 - Train Accuracy: 0.9455, Validation Accuracy: 0.9604, Loss: 0.0403\n",
            "Epoch   1 Batch  500/538 - Train Accuracy: 0.9659, Validation Accuracy: 0.9373, Loss: 0.0301\n",
            "Epoch   1 Batch  510/538 - Train Accuracy: 0.9567, Validation Accuracy: 0.9455, Loss: 0.0358\n",
            "Epoch   1 Batch  520/538 - Train Accuracy: 0.9590, Validation Accuracy: 0.9531, Loss: 0.0391\n",
            "Epoch   1 Batch  530/538 - Train Accuracy: 0.9320, Validation Accuracy: 0.9563, Loss: 0.0434\n",
            "Epoch   2 Batch   10/538 - Train Accuracy: 0.9500, Validation Accuracy: 0.9442, Loss: 0.0419\n",
            "Epoch   2 Batch   20/538 - Train Accuracy: 0.9706, Validation Accuracy: 0.9597, Loss: 0.0344\n",
            "Epoch   2 Batch   30/538 - Train Accuracy: 0.9613, Validation Accuracy: 0.9611, Loss: 0.0412\n",
            "Epoch   2 Batch   40/538 - Train Accuracy: 0.9538, Validation Accuracy: 0.9606, Loss: 0.0278\n",
            "Epoch   2 Batch   50/538 - Train Accuracy: 0.9531, Validation Accuracy: 0.9528, Loss: 0.0332\n",
            "Epoch   2 Batch   60/538 - Train Accuracy: 0.9512, Validation Accuracy: 0.9556, Loss: 0.0325\n",
            "Epoch   2 Batch   70/538 - Train Accuracy: 0.9555, Validation Accuracy: 0.9556, Loss: 0.0305\n",
            "Epoch   2 Batch   80/538 - Train Accuracy: 0.9531, Validation Accuracy: 0.9547, Loss: 0.0362\n",
            "Epoch   2 Batch   90/538 - Train Accuracy: 0.9572, Validation Accuracy: 0.9641, Loss: 0.0428\n",
            "Epoch   2 Batch  100/538 - Train Accuracy: 0.9742, Validation Accuracy: 0.9485, Loss: 0.0265\n",
            "Epoch   2 Batch  110/538 - Train Accuracy: 0.9619, Validation Accuracy: 0.9551, Loss: 0.0381\n",
            "Epoch   2 Batch  120/538 - Train Accuracy: 0.9576, Validation Accuracy: 0.9508, Loss: 0.0260\n",
            "Epoch   2 Batch  130/538 - Train Accuracy: 0.9617, Validation Accuracy: 0.9597, Loss: 0.0340\n",
            "Epoch   2 Batch  140/538 - Train Accuracy: 0.9535, Validation Accuracy: 0.9572, Loss: 0.0401\n",
            "Epoch   2 Batch  150/538 - Train Accuracy: 0.9613, Validation Accuracy: 0.9535, Loss: 0.0319\n",
            "Epoch   2 Batch  160/538 - Train Accuracy: 0.9474, Validation Accuracy: 0.9400, Loss: 0.0324\n",
            "Epoch   2 Batch  170/538 - Train Accuracy: 0.9563, Validation Accuracy: 0.9510, Loss: 0.0335\n",
            "Epoch   2 Batch  180/538 - Train Accuracy: 0.9542, Validation Accuracy: 0.9553, Loss: 0.0350\n",
            "Epoch   2 Batch  190/538 - Train Accuracy: 0.9548, Validation Accuracy: 0.9517, Loss: 0.0416\n",
            "Epoch   2 Batch  200/538 - Train Accuracy: 0.9537, Validation Accuracy: 0.9489, Loss: 0.0296\n",
            "Epoch   2 Batch  210/538 - Train Accuracy: 0.9593, Validation Accuracy: 0.9696, Loss: 0.0375\n",
            "Epoch   2 Batch  220/538 - Train Accuracy: 0.9496, Validation Accuracy: 0.9625, Loss: 0.0371\n",
            "Epoch   2 Batch  230/538 - Train Accuracy: 0.9582, Validation Accuracy: 0.9542, Loss: 0.0303\n",
            "Epoch   2 Batch  240/538 - Train Accuracy: 0.9662, Validation Accuracy: 0.9675, Loss: 0.0329\n",
            "Epoch   2 Batch  250/538 - Train Accuracy: 0.9500, Validation Accuracy: 0.9615, Loss: 0.0296\n",
            "Epoch   2 Batch  260/538 - Train Accuracy: 0.9412, Validation Accuracy: 0.9638, Loss: 0.0320\n",
            "Epoch   2 Batch  270/538 - Train Accuracy: 0.9697, Validation Accuracy: 0.9579, Loss: 0.0245\n",
            "Epoch   2 Batch  280/538 - Train Accuracy: 0.9661, Validation Accuracy: 0.9622, Loss: 0.0240\n",
            "Epoch   2 Batch  290/538 - Train Accuracy: 0.9812, Validation Accuracy: 0.9673, Loss: 0.0283\n",
            "Epoch   2 Batch  300/538 - Train Accuracy: 0.9667, Validation Accuracy: 0.9531, Loss: 0.0289\n",
            "Epoch   2 Batch  310/538 - Train Accuracy: 0.9738, Validation Accuracy: 0.9554, Loss: 0.0348\n",
            "Epoch   2 Batch  320/538 - Train Accuracy: 0.9632, Validation Accuracy: 0.9666, Loss: 0.0263\n",
            "Epoch   2 Batch  330/538 - Train Accuracy: 0.9779, Validation Accuracy: 0.9680, Loss: 0.0285\n",
            "Epoch   2 Batch  340/538 - Train Accuracy: 0.9762, Validation Accuracy: 0.9689, Loss: 0.0282\n",
            "Epoch   2 Batch  350/538 - Train Accuracy: 0.9645, Validation Accuracy: 0.9679, Loss: 0.0319\n",
            "Epoch   2 Batch  360/538 - Train Accuracy: 0.9666, Validation Accuracy: 0.9771, Loss: 0.0219\n",
            "Epoch   2 Batch  370/538 - Train Accuracy: 0.9650, Validation Accuracy: 0.9579, Loss: 0.0298\n",
            "Epoch   2 Batch  380/538 - Train Accuracy: 0.9758, Validation Accuracy: 0.9684, Loss: 0.0235\n",
            "Epoch   2 Batch  390/538 - Train Accuracy: 0.9613, Validation Accuracy: 0.9631, Loss: 0.0250\n",
            "Epoch   2 Batch  400/538 - Train Accuracy: 0.9660, Validation Accuracy: 0.9570, Loss: 0.0275\n",
            "Epoch   2 Batch  410/538 - Train Accuracy: 0.9639, Validation Accuracy: 0.9586, Loss: 0.0269\n",
            "Epoch   2 Batch  420/538 - Train Accuracy: 0.9643, Validation Accuracy: 0.9656, Loss: 0.0287\n",
            "Epoch   2 Batch  430/538 - Train Accuracy: 0.9539, Validation Accuracy: 0.9632, Loss: 0.0317\n",
            "Epoch   2 Batch  440/538 - Train Accuracy: 0.9748, Validation Accuracy: 0.9751, Loss: 0.0278\n",
            "Epoch   2 Batch  450/538 - Train Accuracy: 0.9570, Validation Accuracy: 0.9624, Loss: 0.0367\n",
            "Epoch   2 Batch  460/538 - Train Accuracy: 0.9639, Validation Accuracy: 0.9666, Loss: 0.0251\n",
            "Epoch   2 Batch  470/538 - Train Accuracy: 0.9695, Validation Accuracy: 0.9620, Loss: 0.0278\n",
            "Epoch   2 Batch  480/538 - Train Accuracy: 0.9764, Validation Accuracy: 0.9567, Loss: 0.0210\n",
            "Epoch   2 Batch  490/538 - Train Accuracy: 0.9474, Validation Accuracy: 0.9595, Loss: 0.0282\n",
            "Epoch   2 Batch  500/538 - Train Accuracy: 0.9753, Validation Accuracy: 0.9666, Loss: 0.0178\n",
            "Epoch   2 Batch  510/538 - Train Accuracy: 0.9741, Validation Accuracy: 0.9636, Loss: 0.0235\n",
            "Epoch   2 Batch  520/538 - Train Accuracy: 0.9789, Validation Accuracy: 0.9590, Loss: 0.0248\n",
            "Epoch   2 Batch  530/538 - Train Accuracy: 0.9561, Validation Accuracy: 0.9624, Loss: 0.0282\n",
            "Epoch   3 Batch   10/538 - Train Accuracy: 0.9709, Validation Accuracy: 0.9643, Loss: 0.0250\n",
            "Epoch   3 Batch   20/538 - Train Accuracy: 0.9740, Validation Accuracy: 0.9609, Loss: 0.0259\n",
            "Epoch   3 Batch   30/538 - Train Accuracy: 0.9643, Validation Accuracy: 0.9577, Loss: 0.0278\n",
            "Epoch   3 Batch   40/538 - Train Accuracy: 0.9737, Validation Accuracy: 0.9680, Loss: 0.0207\n",
            "Epoch   3 Batch   50/538 - Train Accuracy: 0.9732, Validation Accuracy: 0.9672, Loss: 0.0212\n",
            "Epoch   3 Batch   60/538 - Train Accuracy: 0.9760, Validation Accuracy: 0.9691, Loss: 0.0229\n",
            "Epoch   3 Batch   70/538 - Train Accuracy: 0.9702, Validation Accuracy: 0.9744, Loss: 0.0201\n",
            "Epoch   3 Batch   80/538 - Train Accuracy: 0.9545, Validation Accuracy: 0.9702, Loss: 0.0233\n",
            "Epoch   3 Batch   90/538 - Train Accuracy: 0.9676, Validation Accuracy: 0.9703, Loss: 0.0282\n",
            "Epoch   3 Batch  100/538 - Train Accuracy: 0.9791, Validation Accuracy: 0.9624, Loss: 0.0184\n",
            "Epoch   3 Batch  110/538 - Train Accuracy: 0.9770, Validation Accuracy: 0.9599, Loss: 0.0213\n",
            "Epoch   3 Batch  120/538 - Train Accuracy: 0.9773, Validation Accuracy: 0.9675, Loss: 0.0203\n",
            "Epoch   3 Batch  130/538 - Train Accuracy: 0.9723, Validation Accuracy: 0.9609, Loss: 0.0216\n",
            "Epoch   3 Batch  140/538 - Train Accuracy: 0.9549, Validation Accuracy: 0.9554, Loss: 0.0317\n",
            "Epoch   3 Batch  150/538 - Train Accuracy: 0.9799, Validation Accuracy: 0.9737, Loss: 0.0183\n",
            "Epoch   3 Batch  160/538 - Train Accuracy: 0.9710, Validation Accuracy: 0.9647, Loss: 0.0182\n",
            "Epoch   3 Batch  170/538 - Train Accuracy: 0.9702, Validation Accuracy: 0.9714, Loss: 0.0233\n",
            "Epoch   3 Batch  180/538 - Train Accuracy: 0.9738, Validation Accuracy: 0.9553, Loss: 0.0216\n",
            "Epoch   3 Batch  190/538 - Train Accuracy: 0.9585, Validation Accuracy: 0.9629, Loss: 0.0320\n",
            "Epoch   3 Batch  200/538 - Train Accuracy: 0.9744, Validation Accuracy: 0.9808, Loss: 0.0200\n",
            "Epoch   3 Batch  210/538 - Train Accuracy: 0.9730, Validation Accuracy: 0.9746, Loss: 0.0218\n",
            "Epoch   3 Batch  220/538 - Train Accuracy: 0.9660, Validation Accuracy: 0.9789, Loss: 0.0233\n",
            "Epoch   3 Batch  230/538 - Train Accuracy: 0.9775, Validation Accuracy: 0.9668, Loss: 0.0196\n",
            "Epoch   3 Batch  240/538 - Train Accuracy: 0.9643, Validation Accuracy: 0.9734, Loss: 0.0211\n",
            "Epoch   3 Batch  250/538 - Train Accuracy: 0.9760, Validation Accuracy: 0.9643, Loss: 0.0212\n",
            "Epoch   3 Batch  260/538 - Train Accuracy: 0.9544, Validation Accuracy: 0.9714, Loss: 0.0237\n",
            "Epoch   3 Batch  270/538 - Train Accuracy: 0.9732, Validation Accuracy: 0.9705, Loss: 0.0188\n",
            "Epoch   3 Batch  280/538 - Train Accuracy: 0.9823, Validation Accuracy: 0.9700, Loss: 0.0168\n",
            "Epoch   3 Batch  290/538 - Train Accuracy: 0.9799, Validation Accuracy: 0.9664, Loss: 0.0171\n",
            "Epoch   3 Batch  300/538 - Train Accuracy: 0.9706, Validation Accuracy: 0.9748, Loss: 0.0241\n",
            "Epoch   3 Batch  310/538 - Train Accuracy: 0.9826, Validation Accuracy: 0.9629, Loss: 0.0213\n",
            "Epoch   3 Batch  320/538 - Train Accuracy: 0.9812, Validation Accuracy: 0.9659, Loss: 0.0202\n",
            "Epoch   3 Batch  330/538 - Train Accuracy: 0.9795, Validation Accuracy: 0.9592, Loss: 0.0192\n",
            "Epoch   3 Batch  340/538 - Train Accuracy: 0.9701, Validation Accuracy: 0.9661, Loss: 0.0197\n",
            "Epoch   3 Batch  350/538 - Train Accuracy: 0.9788, Validation Accuracy: 0.9588, Loss: 0.0243\n",
            "Epoch   3 Batch  360/538 - Train Accuracy: 0.9791, Validation Accuracy: 0.9707, Loss: 0.0144\n",
            "Epoch   3 Batch  370/538 - Train Accuracy: 0.9758, Validation Accuracy: 0.9682, Loss: 0.0198\n",
            "Epoch   3 Batch  380/538 - Train Accuracy: 0.9773, Validation Accuracy: 0.9684, Loss: 0.0178\n",
            "Epoch   3 Batch  390/538 - Train Accuracy: 0.9786, Validation Accuracy: 0.9586, Loss: 0.0174\n",
            "Epoch   3 Batch  400/538 - Train Accuracy: 0.9775, Validation Accuracy: 0.9766, Loss: 0.0171\n",
            "Epoch   3 Batch  410/538 - Train Accuracy: 0.9789, Validation Accuracy: 0.9618, Loss: 0.0174\n",
            "Epoch   3 Batch  420/538 - Train Accuracy: 0.9682, Validation Accuracy: 0.9616, Loss: 0.0192\n",
            "Epoch   3 Batch  430/538 - Train Accuracy: 0.9584, Validation Accuracy: 0.9748, Loss: 0.0208\n",
            "Epoch   3 Batch  440/538 - Train Accuracy: 0.9869, Validation Accuracy: 0.9730, Loss: 0.0208\n",
            "Epoch   3 Batch  450/538 - Train Accuracy: 0.9695, Validation Accuracy: 0.9712, Loss: 0.0282\n",
            "Epoch   3 Batch  460/538 - Train Accuracy: 0.9645, Validation Accuracy: 0.9716, Loss: 0.0228\n",
            "Epoch   3 Batch  470/538 - Train Accuracy: 0.9725, Validation Accuracy: 0.9663, Loss: 0.0231\n",
            "Epoch   3 Batch  480/538 - Train Accuracy: 0.9827, Validation Accuracy: 0.9634, Loss: 0.0188\n",
            "Epoch   3 Batch  490/538 - Train Accuracy: 0.9667, Validation Accuracy: 0.9615, Loss: 0.0189\n",
            "Epoch   3 Batch  500/538 - Train Accuracy: 0.9877, Validation Accuracy: 0.9711, Loss: 0.0132\n",
            "Epoch   3 Batch  510/538 - Train Accuracy: 0.9851, Validation Accuracy: 0.9641, Loss: 0.0178\n",
            "Epoch   3 Batch  520/538 - Train Accuracy: 0.9748, Validation Accuracy: 0.9737, Loss: 0.0181\n",
            "Epoch   3 Batch  530/538 - Train Accuracy: 0.9820, Validation Accuracy: 0.9696, Loss: 0.0233\n",
            "Epoch   4 Batch   10/538 - Train Accuracy: 0.9828, Validation Accuracy: 0.9689, Loss: 0.0173\n",
            "Epoch   4 Batch   20/538 - Train Accuracy: 0.9749, Validation Accuracy: 0.9691, Loss: 0.0204\n",
            "Epoch   4 Batch   30/538 - Train Accuracy: 0.9617, Validation Accuracy: 0.9682, Loss: 0.0181\n",
            "Epoch   4 Batch   40/538 - Train Accuracy: 0.9760, Validation Accuracy: 0.9762, Loss: 0.0148\n",
            "Epoch   4 Batch   50/538 - Train Accuracy: 0.9717, Validation Accuracy: 0.9716, Loss: 0.0168\n",
            "Epoch   4 Batch   60/538 - Train Accuracy: 0.9824, Validation Accuracy: 0.9707, Loss: 0.0181\n",
            "Epoch   4 Batch   70/538 - Train Accuracy: 0.9749, Validation Accuracy: 0.9789, Loss: 0.0151\n",
            "Epoch   4 Batch   80/538 - Train Accuracy: 0.9773, Validation Accuracy: 0.9703, Loss: 0.0159\n",
            "Epoch   4 Batch   90/538 - Train Accuracy: 0.9790, Validation Accuracy: 0.9620, Loss: 0.0198\n",
            "Epoch   4 Batch  100/538 - Train Accuracy: 0.9812, Validation Accuracy: 0.9693, Loss: 0.0144\n",
            "Epoch   4 Batch  110/538 - Train Accuracy: 0.9783, Validation Accuracy: 0.9640, Loss: 0.0188\n",
            "Epoch   4 Batch  120/538 - Train Accuracy: 0.9838, Validation Accuracy: 0.9705, Loss: 0.0135\n",
            "Epoch   4 Batch  130/538 - Train Accuracy: 0.9706, Validation Accuracy: 0.9698, Loss: 0.0216\n",
            "Epoch   4 Batch  140/538 - Train Accuracy: 0.9760, Validation Accuracy: 0.9790, Loss: 0.0230\n",
            "Epoch   4 Batch  150/538 - Train Accuracy: 0.9787, Validation Accuracy: 0.9716, Loss: 0.0170\n",
            "Epoch   4 Batch  160/538 - Train Accuracy: 0.9674, Validation Accuracy: 0.9611, Loss: 0.0163\n",
            "Epoch   4 Batch  170/538 - Train Accuracy: 0.9717, Validation Accuracy: 0.9718, Loss: 0.0216\n",
            "Epoch   4 Batch  180/538 - Train Accuracy: 0.9754, Validation Accuracy: 0.9737, Loss: 0.0177\n",
            "Epoch   4 Batch  190/538 - Train Accuracy: 0.9701, Validation Accuracy: 0.9657, Loss: 0.0213\n",
            "Epoch   4 Batch  200/538 - Train Accuracy: 0.9861, Validation Accuracy: 0.9725, Loss: 0.0134\n",
            "Epoch   4 Batch  210/538 - Train Accuracy: 0.9857, Validation Accuracy: 0.9812, Loss: 0.0163\n",
            "Epoch   4 Batch  220/538 - Train Accuracy: 0.9634, Validation Accuracy: 0.9775, Loss: 0.0178\n",
            "Epoch   4 Batch  230/538 - Train Accuracy: 0.9777, Validation Accuracy: 0.9712, Loss: 0.0186\n",
            "Epoch   4 Batch  240/538 - Train Accuracy: 0.9775, Validation Accuracy: 0.9721, Loss: 0.0166\n",
            "Epoch   4 Batch  250/538 - Train Accuracy: 0.9848, Validation Accuracy: 0.9677, Loss: 0.0169\n",
            "Epoch   4 Batch  260/538 - Train Accuracy: 0.9650, Validation Accuracy: 0.9757, Loss: 0.0179\n",
            "Epoch   4 Batch  270/538 - Train Accuracy: 0.9828, Validation Accuracy: 0.9759, Loss: 0.0154\n",
            "Epoch   4 Batch  280/538 - Train Accuracy: 0.9877, Validation Accuracy: 0.9739, Loss: 0.0124\n",
            "Epoch   4 Batch  290/538 - Train Accuracy: 0.9895, Validation Accuracy: 0.9723, Loss: 0.0149\n",
            "Epoch   4 Batch  300/538 - Train Accuracy: 0.9840, Validation Accuracy: 0.9686, Loss: 0.0156\n",
            "Epoch   4 Batch  310/538 - Train Accuracy: 0.9869, Validation Accuracy: 0.9737, Loss: 0.0188\n",
            "Epoch   4 Batch  320/538 - Train Accuracy: 0.9786, Validation Accuracy: 0.9775, Loss: 0.0131\n",
            "Epoch   4 Batch  330/538 - Train Accuracy: 0.9803, Validation Accuracy: 0.9645, Loss: 0.0151\n",
            "Epoch   4 Batch  340/538 - Train Accuracy: 0.9812, Validation Accuracy: 0.9734, Loss: 0.0154\n",
            "Epoch   4 Batch  350/538 - Train Accuracy: 0.9790, Validation Accuracy: 0.9695, Loss: 0.0171\n",
            "Epoch   4 Batch  360/538 - Train Accuracy: 0.9818, Validation Accuracy: 0.9814, Loss: 0.0111\n",
            "Epoch   4 Batch  370/538 - Train Accuracy: 0.9787, Validation Accuracy: 0.9657, Loss: 0.0152\n",
            "Epoch   4 Batch  380/538 - Train Accuracy: 0.9873, Validation Accuracy: 0.9634, Loss: 0.0137\n",
            "Epoch   4 Batch  390/538 - Train Accuracy: 0.9749, Validation Accuracy: 0.9643, Loss: 0.0134\n",
            "Epoch   4 Batch  400/538 - Train Accuracy: 0.9866, Validation Accuracy: 0.9755, Loss: 0.0127\n",
            "Epoch   4 Batch  410/538 - Train Accuracy: 0.9893, Validation Accuracy: 0.9776, Loss: 0.0122\n",
            "Epoch   4 Batch  420/538 - Train Accuracy: 0.9795, Validation Accuracy: 0.9709, Loss: 0.0183\n",
            "Epoch   4 Batch  430/538 - Train Accuracy: 0.9734, Validation Accuracy: 0.9785, Loss: 0.0160\n",
            "Epoch   4 Batch  440/538 - Train Accuracy: 0.9863, Validation Accuracy: 0.9741, Loss: 0.0124\n",
            "Epoch   4 Batch  450/538 - Train Accuracy: 0.9676, Validation Accuracy: 0.9702, Loss: 0.0226\n",
            "Epoch   4 Batch  460/538 - Train Accuracy: 0.9788, Validation Accuracy: 0.9808, Loss: 0.0176\n",
            "Epoch   4 Batch  470/538 - Train Accuracy: 0.9760, Validation Accuracy: 0.9750, Loss: 0.0177\n",
            "Epoch   4 Batch  480/538 - Train Accuracy: 0.9868, Validation Accuracy: 0.9691, Loss: 0.0109\n",
            "Epoch   4 Batch  490/538 - Train Accuracy: 0.9743, Validation Accuracy: 0.9755, Loss: 0.0159\n",
            "Epoch   4 Batch  500/538 - Train Accuracy: 0.9886, Validation Accuracy: 0.9824, Loss: 0.0100\n",
            "Epoch   4 Batch  510/538 - Train Accuracy: 0.9801, Validation Accuracy: 0.9727, Loss: 0.0150\n",
            "Epoch   4 Batch  520/538 - Train Accuracy: 0.9756, Validation Accuracy: 0.9634, Loss: 0.0160\n",
            "Epoch   4 Batch  530/538 - Train Accuracy: 0.9717, Validation Accuracy: 0.9585, Loss: 0.0178\n",
            "Epoch   5 Batch   10/538 - Train Accuracy: 0.9869, Validation Accuracy: 0.9746, Loss: 0.0148\n",
            "Epoch   5 Batch   20/538 - Train Accuracy: 0.9749, Validation Accuracy: 0.9776, Loss: 0.0178\n",
            "Epoch   5 Batch   30/538 - Train Accuracy: 0.9793, Validation Accuracy: 0.9782, Loss: 0.0195\n",
            "Epoch   5 Batch   40/538 - Train Accuracy: 0.9750, Validation Accuracy: 0.9727, Loss: 0.0138\n",
            "Epoch   5 Batch   50/538 - Train Accuracy: 0.9768, Validation Accuracy: 0.9679, Loss: 0.0145\n",
            "Epoch   5 Batch   60/538 - Train Accuracy: 0.9805, Validation Accuracy: 0.9773, Loss: 0.0148\n",
            "Epoch   5 Batch   70/538 - Train Accuracy: 0.9860, Validation Accuracy: 0.9782, Loss: 0.0128\n",
            "Epoch   5 Batch   80/538 - Train Accuracy: 0.9891, Validation Accuracy: 0.9737, Loss: 0.0104\n",
            "Epoch   5 Batch   90/538 - Train Accuracy: 0.9803, Validation Accuracy: 0.9659, Loss: 0.0127\n",
            "Epoch   5 Batch  100/538 - Train Accuracy: 0.9910, Validation Accuracy: 0.9808, Loss: 0.0099\n",
            "Epoch   5 Batch  110/538 - Train Accuracy: 0.9807, Validation Accuracy: 0.9762, Loss: 0.0117\n",
            "Epoch   5 Batch  120/538 - Train Accuracy: 0.9812, Validation Accuracy: 0.9782, Loss: 0.0139\n",
            "Epoch   5 Batch  130/538 - Train Accuracy: 0.9814, Validation Accuracy: 0.9727, Loss: 0.0172\n",
            "Epoch   5 Batch  140/538 - Train Accuracy: 0.9879, Validation Accuracy: 0.9790, Loss: 0.0161\n",
            "Epoch   5 Batch  150/538 - Train Accuracy: 0.9867, Validation Accuracy: 0.9796, Loss: 0.0127\n",
            "Epoch   5 Batch  160/538 - Train Accuracy: 0.9851, Validation Accuracy: 0.9686, Loss: 0.0144\n",
            "Epoch   5 Batch  170/538 - Train Accuracy: 0.9805, Validation Accuracy: 0.9688, Loss: 0.0167\n",
            "Epoch   5 Batch  180/538 - Train Accuracy: 0.9740, Validation Accuracy: 0.9712, Loss: 0.0151\n",
            "Epoch   5 Batch  190/538 - Train Accuracy: 0.9786, Validation Accuracy: 0.9634, Loss: 0.0149\n",
            "Epoch   5 Batch  200/538 - Train Accuracy: 0.9826, Validation Accuracy: 0.9730, Loss: 0.0110\n",
            "Epoch   5 Batch  210/538 - Train Accuracy: 0.9849, Validation Accuracy: 0.9771, Loss: 0.0159\n",
            "Epoch   5 Batch  220/538 - Train Accuracy: 0.9788, Validation Accuracy: 0.9702, Loss: 0.0145\n",
            "Epoch   5 Batch  230/538 - Train Accuracy: 0.9748, Validation Accuracy: 0.9668, Loss: 0.0163\n",
            "Epoch   5 Batch  240/538 - Train Accuracy: 0.9729, Validation Accuracy: 0.9707, Loss: 0.0152\n",
            "Epoch   5 Batch  250/538 - Train Accuracy: 0.9840, Validation Accuracy: 0.9620, Loss: 0.0146\n",
            "Epoch   5 Batch  260/538 - Train Accuracy: 0.9650, Validation Accuracy: 0.9600, Loss: 0.0169\n",
            "Epoch   5 Batch  270/538 - Train Accuracy: 0.9842, Validation Accuracy: 0.9739, Loss: 0.0123\n",
            "Epoch   5 Batch  280/538 - Train Accuracy: 0.9866, Validation Accuracy: 0.9725, Loss: 0.0118\n",
            "Epoch   5 Batch  290/538 - Train Accuracy: 0.9896, Validation Accuracy: 0.9806, Loss: 0.0106\n",
            "Epoch   5 Batch  300/538 - Train Accuracy: 0.9751, Validation Accuracy: 0.9698, Loss: 0.0126\n",
            "Epoch   5 Batch  310/538 - Train Accuracy: 0.9926, Validation Accuracy: 0.9803, Loss: 0.0127\n",
            "Epoch   5 Batch  320/538 - Train Accuracy: 0.9887, Validation Accuracy: 0.9757, Loss: 0.0123\n",
            "Epoch   5 Batch  330/538 - Train Accuracy: 0.9872, Validation Accuracy: 0.9691, Loss: 0.0112\n",
            "Epoch   5 Batch  340/538 - Train Accuracy: 0.9834, Validation Accuracy: 0.9709, Loss: 0.0108\n",
            "Epoch   5 Batch  350/538 - Train Accuracy: 0.9909, Validation Accuracy: 0.9652, Loss: 0.0175\n",
            "Epoch   5 Batch  360/538 - Train Accuracy: 0.9889, Validation Accuracy: 0.9711, Loss: 0.0118\n",
            "Epoch   5 Batch  370/538 - Train Accuracy: 0.9852, Validation Accuracy: 0.9640, Loss: 0.0124\n",
            "Epoch   5 Batch  380/538 - Train Accuracy: 0.9836, Validation Accuracy: 0.9661, Loss: 0.0145\n",
            "Epoch   5 Batch  390/538 - Train Accuracy: 0.9849, Validation Accuracy: 0.9677, Loss: 0.0097\n",
            "Epoch   5 Batch  400/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9746, Loss: 0.0119\n",
            "Epoch   5 Batch  410/538 - Train Accuracy: 0.9906, Validation Accuracy: 0.9723, Loss: 0.0132\n",
            "Epoch   5 Batch  420/538 - Train Accuracy: 0.9855, Validation Accuracy: 0.9735, Loss: 0.0124\n",
            "Epoch   5 Batch  430/538 - Train Accuracy: 0.9832, Validation Accuracy: 0.9743, Loss: 0.0116\n",
            "Epoch   5 Batch  440/538 - Train Accuracy: 0.9902, Validation Accuracy: 0.9757, Loss: 0.0151\n",
            "Epoch   5 Batch  450/538 - Train Accuracy: 0.9771, Validation Accuracy: 0.9707, Loss: 0.0221\n",
            "Epoch   5 Batch  460/538 - Train Accuracy: 0.9797, Validation Accuracy: 0.9842, Loss: 0.0154\n",
            "Epoch   5 Batch  470/538 - Train Accuracy: 0.9833, Validation Accuracy: 0.9744, Loss: 0.0161\n",
            "Epoch   5 Batch  480/538 - Train Accuracy: 0.9844, Validation Accuracy: 0.9741, Loss: 0.0118\n",
            "Epoch   5 Batch  490/538 - Train Accuracy: 0.9816, Validation Accuracy: 0.9842, Loss: 0.0124\n",
            "Epoch   5 Batch  500/538 - Train Accuracy: 0.9922, Validation Accuracy: 0.9769, Loss: 0.0105\n",
            "Epoch   5 Batch  510/538 - Train Accuracy: 0.9825, Validation Accuracy: 0.9689, Loss: 0.0115\n",
            "Epoch   5 Batch  520/538 - Train Accuracy: 0.9893, Validation Accuracy: 0.9688, Loss: 0.0143\n",
            "Epoch   5 Batch  530/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9727, Loss: 0.0135\n",
            "Epoch   6 Batch   10/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9762, Loss: 0.0101\n",
            "Epoch   6 Batch   20/538 - Train Accuracy: 0.9888, Validation Accuracy: 0.9741, Loss: 0.0183\n",
            "Epoch   6 Batch   30/538 - Train Accuracy: 0.9789, Validation Accuracy: 0.9792, Loss: 0.0164\n",
            "Epoch   6 Batch   40/538 - Train Accuracy: 0.9842, Validation Accuracy: 0.9815, Loss: 0.0112\n",
            "Epoch   6 Batch   50/538 - Train Accuracy: 0.9889, Validation Accuracy: 0.9757, Loss: 0.0103\n",
            "Epoch   6 Batch   60/538 - Train Accuracy: 0.9973, Validation Accuracy: 0.9792, Loss: 0.0109\n",
            "Epoch   6 Batch   70/538 - Train Accuracy: 0.9844, Validation Accuracy: 0.9824, Loss: 0.0119\n",
            "Epoch   6 Batch   80/538 - Train Accuracy: 0.9857, Validation Accuracy: 0.9773, Loss: 0.0087\n",
            "Epoch   6 Batch   90/538 - Train Accuracy: 0.9771, Validation Accuracy: 0.9810, Loss: 0.0133\n",
            "Epoch   6 Batch  100/538 - Train Accuracy: 0.9906, Validation Accuracy: 0.9833, Loss: 0.0110\n",
            "Epoch   6 Batch  110/538 - Train Accuracy: 0.9861, Validation Accuracy: 0.9718, Loss: 0.0114\n",
            "Epoch   6 Batch  120/538 - Train Accuracy: 0.9891, Validation Accuracy: 0.9789, Loss: 0.0075\n",
            "Epoch   6 Batch  130/538 - Train Accuracy: 0.9829, Validation Accuracy: 0.9831, Loss: 0.0113\n",
            "Epoch   6 Batch  140/538 - Train Accuracy: 0.9773, Validation Accuracy: 0.9744, Loss: 0.0137\n",
            "Epoch   6 Batch  150/538 - Train Accuracy: 0.9820, Validation Accuracy: 0.9746, Loss: 0.0105\n",
            "Epoch   6 Batch  160/538 - Train Accuracy: 0.9862, Validation Accuracy: 0.9789, Loss: 0.0112\n",
            "Epoch   6 Batch  170/538 - Train Accuracy: 0.9868, Validation Accuracy: 0.9682, Loss: 0.0117\n",
            "Epoch   6 Batch  180/538 - Train Accuracy: 0.9818, Validation Accuracy: 0.9624, Loss: 0.0131\n",
            "Epoch   6 Batch  190/538 - Train Accuracy: 0.9860, Validation Accuracy: 0.9762, Loss: 0.0151\n",
            "Epoch   6 Batch  200/538 - Train Accuracy: 0.9787, Validation Accuracy: 0.9817, Loss: 0.0127\n",
            "Epoch   6 Batch  210/538 - Train Accuracy: 0.9913, Validation Accuracy: 0.9759, Loss: 0.0119\n",
            "Epoch   6 Batch  220/538 - Train Accuracy: 0.9855, Validation Accuracy: 0.9709, Loss: 0.0161\n",
            "Epoch   6 Batch  230/538 - Train Accuracy: 0.9828, Validation Accuracy: 0.9743, Loss: 0.0109\n",
            "Epoch   6 Batch  240/538 - Train Accuracy: 0.9785, Validation Accuracy: 0.9688, Loss: 0.0144\n",
            "Epoch   6 Batch  250/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9707, Loss: 0.0107\n",
            "Epoch   6 Batch  260/538 - Train Accuracy: 0.9771, Validation Accuracy: 0.9764, Loss: 0.0131\n",
            "Epoch   6 Batch  270/538 - Train Accuracy: 0.9789, Validation Accuracy: 0.9728, Loss: 0.0130\n",
            "Epoch   6 Batch  280/538 - Train Accuracy: 0.9900, Validation Accuracy: 0.9719, Loss: 0.0113\n",
            "Epoch   6 Batch  290/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9769, Loss: 0.0122\n",
            "Epoch   6 Batch  300/538 - Train Accuracy: 0.9896, Validation Accuracy: 0.9883, Loss: 0.0111\n",
            "Epoch   6 Batch  310/538 - Train Accuracy: 0.9938, Validation Accuracy: 0.9790, Loss: 0.0119\n",
            "Epoch   6 Batch  320/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9767, Loss: 0.0107\n",
            "Epoch   6 Batch  330/538 - Train Accuracy: 0.9913, Validation Accuracy: 0.9764, Loss: 0.0105\n",
            "Epoch   6 Batch  340/538 - Train Accuracy: 0.9889, Validation Accuracy: 0.9783, Loss: 0.0093\n",
            "Epoch   6 Batch  350/538 - Train Accuracy: 0.9855, Validation Accuracy: 0.9686, Loss: 0.0131\n",
            "Epoch   6 Batch  360/538 - Train Accuracy: 0.9875, Validation Accuracy: 0.9801, Loss: 0.0073\n",
            "Epoch   6 Batch  370/538 - Train Accuracy: 0.9801, Validation Accuracy: 0.9812, Loss: 0.0124\n",
            "Epoch   6 Batch  380/538 - Train Accuracy: 0.9879, Validation Accuracy: 0.9785, Loss: 0.0101\n",
            "Epoch   6 Batch  390/538 - Train Accuracy: 0.9775, Validation Accuracy: 0.9727, Loss: 0.0110\n",
            "Epoch   6 Batch  400/538 - Train Accuracy: 0.9909, Validation Accuracy: 0.9858, Loss: 0.0094\n",
            "Epoch   6 Batch  410/538 - Train Accuracy: 0.9947, Validation Accuracy: 0.9691, Loss: 0.0096\n",
            "Epoch   6 Batch  420/538 - Train Accuracy: 0.9795, Validation Accuracy: 0.9727, Loss: 0.0135\n",
            "Epoch   6 Batch  430/538 - Train Accuracy: 0.9793, Validation Accuracy: 0.9780, Loss: 0.0158\n",
            "Epoch   6 Batch  440/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9723, Loss: 0.0181\n",
            "Epoch   6 Batch  450/538 - Train Accuracy: 0.9751, Validation Accuracy: 0.9764, Loss: 0.0171\n",
            "Epoch   6 Batch  460/538 - Train Accuracy: 0.9823, Validation Accuracy: 0.9773, Loss: 0.0148\n",
            "Epoch   6 Batch  470/538 - Train Accuracy: 0.9808, Validation Accuracy: 0.9663, Loss: 0.0141\n",
            "Epoch   6 Batch  480/538 - Train Accuracy: 0.9862, Validation Accuracy: 0.9817, Loss: 0.0103\n",
            "Epoch   6 Batch  490/538 - Train Accuracy: 0.9825, Validation Accuracy: 0.9824, Loss: 0.0115\n",
            "Epoch   6 Batch  500/538 - Train Accuracy: 0.9911, Validation Accuracy: 0.9776, Loss: 0.0103\n",
            "Epoch   6 Batch  510/538 - Train Accuracy: 0.9734, Validation Accuracy: 0.9759, Loss: 0.0132\n",
            "Epoch   6 Batch  520/538 - Train Accuracy: 0.9910, Validation Accuracy: 0.9746, Loss: 0.0143\n",
            "Epoch   6 Batch  530/538 - Train Accuracy: 0.9848, Validation Accuracy: 0.9803, Loss: 0.0121\n",
            "Epoch   7 Batch   10/538 - Train Accuracy: 0.9889, Validation Accuracy: 0.9735, Loss: 0.0121\n",
            "Epoch   7 Batch   20/538 - Train Accuracy: 0.9857, Validation Accuracy: 0.9798, Loss: 0.0132\n",
            "Epoch   7 Batch   30/538 - Train Accuracy: 0.9873, Validation Accuracy: 0.9707, Loss: 0.0094\n",
            "Epoch   7 Batch   40/538 - Train Accuracy: 0.9954, Validation Accuracy: 0.9709, Loss: 0.0086\n",
            "Epoch   7 Batch   50/538 - Train Accuracy: 0.9824, Validation Accuracy: 0.9707, Loss: 0.0126\n",
            "Epoch   7 Batch   60/538 - Train Accuracy: 0.9822, Validation Accuracy: 0.9826, Loss: 0.0120\n",
            "Epoch   7 Batch   70/538 - Train Accuracy: 0.9875, Validation Accuracy: 0.9805, Loss: 0.0105\n",
            "Epoch   7 Batch   80/538 - Train Accuracy: 0.9930, Validation Accuracy: 0.9727, Loss: 0.0085\n",
            "Epoch   7 Batch   90/538 - Train Accuracy: 0.9773, Validation Accuracy: 0.9753, Loss: 0.0168\n",
            "Epoch   7 Batch  100/538 - Train Accuracy: 0.9875, Validation Accuracy: 0.9842, Loss: 0.0075\n",
            "Epoch   7 Batch  110/538 - Train Accuracy: 0.9881, Validation Accuracy: 0.9608, Loss: 0.0109\n",
            "Epoch   7 Batch  120/538 - Train Accuracy: 0.9936, Validation Accuracy: 0.9844, Loss: 0.0086\n",
            "Epoch   7 Batch  130/538 - Train Accuracy: 0.9892, Validation Accuracy: 0.9735, Loss: 0.0100\n",
            "Epoch   7 Batch  140/538 - Train Accuracy: 0.9791, Validation Accuracy: 0.9695, Loss: 0.0129\n",
            "Epoch   7 Batch  150/538 - Train Accuracy: 0.9910, Validation Accuracy: 0.9727, Loss: 0.0094\n",
            "Epoch   7 Batch  160/538 - Train Accuracy: 0.9868, Validation Accuracy: 0.9783, Loss: 0.0107\n",
            "Epoch   7 Batch  170/538 - Train Accuracy: 0.9846, Validation Accuracy: 0.9711, Loss: 0.0107\n",
            "Epoch   7 Batch  180/538 - Train Accuracy: 0.9829, Validation Accuracy: 0.9714, Loss: 0.0124\n",
            "Epoch   7 Batch  190/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9759, Loss: 0.0115\n",
            "Epoch   7 Batch  200/538 - Train Accuracy: 0.9875, Validation Accuracy: 0.9805, Loss: 0.0092\n",
            "Epoch   7 Batch  210/538 - Train Accuracy: 0.9883, Validation Accuracy: 0.9737, Loss: 0.0094\n",
            "Epoch   7 Batch  220/538 - Train Accuracy: 0.9870, Validation Accuracy: 0.9760, Loss: 0.0110\n",
            "Epoch   7 Batch  230/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9759, Loss: 0.0103\n",
            "Epoch   7 Batch  240/538 - Train Accuracy: 0.9854, Validation Accuracy: 0.9782, Loss: 0.0140\n",
            "Epoch   7 Batch  250/538 - Train Accuracy: 0.9871, Validation Accuracy: 0.9696, Loss: 0.0108\n",
            "Epoch   7 Batch  260/538 - Train Accuracy: 0.9719, Validation Accuracy: 0.9775, Loss: 0.0163\n",
            "Epoch   7 Batch  270/538 - Train Accuracy: 0.9943, Validation Accuracy: 0.9702, Loss: 0.0090\n",
            "Epoch   7 Batch  280/538 - Train Accuracy: 0.9905, Validation Accuracy: 0.9689, Loss: 0.0106\n",
            "Epoch   7 Batch  290/538 - Train Accuracy: 0.9920, Validation Accuracy: 0.9767, Loss: 0.0101\n",
            "Epoch   7 Batch  300/538 - Train Accuracy: 0.9840, Validation Accuracy: 0.9732, Loss: 0.0115\n",
            "Epoch   7 Batch  310/538 - Train Accuracy: 0.9879, Validation Accuracy: 0.9739, Loss: 0.0114\n",
            "Epoch   7 Batch  320/538 - Train Accuracy: 0.9940, Validation Accuracy: 0.9739, Loss: 0.0071\n",
            "Epoch   7 Batch  330/538 - Train Accuracy: 0.9926, Validation Accuracy: 0.9755, Loss: 0.0089\n",
            "Epoch   7 Batch  340/538 - Train Accuracy: 0.9924, Validation Accuracy: 0.9703, Loss: 0.0088\n",
            "Epoch   7 Batch  350/538 - Train Accuracy: 0.9844, Validation Accuracy: 0.9762, Loss: 0.0120\n",
            "Epoch   7 Batch  360/538 - Train Accuracy: 0.9887, Validation Accuracy: 0.9744, Loss: 0.0081\n",
            "Epoch   7 Batch  370/538 - Train Accuracy: 0.9883, Validation Accuracy: 0.9753, Loss: 0.0084\n",
            "Epoch   7 Batch  380/538 - Train Accuracy: 0.9852, Validation Accuracy: 0.9783, Loss: 0.0106\n",
            "Epoch   7 Batch  390/538 - Train Accuracy: 0.9849, Validation Accuracy: 0.9703, Loss: 0.0082\n",
            "Epoch   7 Batch  400/538 - Train Accuracy: 0.9864, Validation Accuracy: 0.9771, Loss: 0.0107\n",
            "Epoch   7 Batch  410/538 - Train Accuracy: 0.9869, Validation Accuracy: 0.9757, Loss: 0.0097\n",
            "Epoch   7 Batch  420/538 - Train Accuracy: 0.9873, Validation Accuracy: 0.9743, Loss: 0.0105\n",
            "Epoch   7 Batch  430/538 - Train Accuracy: 0.9865, Validation Accuracy: 0.9787, Loss: 0.0147\n",
            "Epoch   7 Batch  440/538 - Train Accuracy: 0.9904, Validation Accuracy: 0.9799, Loss: 0.0114\n",
            "Epoch   7 Batch  450/538 - Train Accuracy: 0.9801, Validation Accuracy: 0.9755, Loss: 0.0138\n",
            "Epoch   7 Batch  460/538 - Train Accuracy: 0.9874, Validation Accuracy: 0.9735, Loss: 0.0143\n",
            "Epoch   7 Batch  470/538 - Train Accuracy: 0.9820, Validation Accuracy: 0.9789, Loss: 0.0126\n",
            "Epoch   7 Batch  480/538 - Train Accuracy: 0.9935, Validation Accuracy: 0.9789, Loss: 0.0073\n",
            "Epoch   7 Batch  490/538 - Train Accuracy: 0.9836, Validation Accuracy: 0.9785, Loss: 0.0109\n",
            "Epoch   7 Batch  500/538 - Train Accuracy: 0.9968, Validation Accuracy: 0.9725, Loss: 0.0068\n",
            "Epoch   7 Batch  510/538 - Train Accuracy: 0.9981, Validation Accuracy: 0.9778, Loss: 0.0078\n",
            "Epoch   7 Batch  520/538 - Train Accuracy: 0.9898, Validation Accuracy: 0.9698, Loss: 0.0124\n",
            "Epoch   7 Batch  530/538 - Train Accuracy: 0.9887, Validation Accuracy: 0.9684, Loss: 0.0093\n",
            "Epoch   8 Batch   10/538 - Train Accuracy: 0.9975, Validation Accuracy: 0.9787, Loss: 0.0117\n",
            "Epoch   8 Batch   20/538 - Train Accuracy: 0.9890, Validation Accuracy: 0.9735, Loss: 0.0117\n",
            "Epoch   8 Batch   30/538 - Train Accuracy: 0.9850, Validation Accuracy: 0.9755, Loss: 0.0100\n",
            "Epoch   8 Batch   40/538 - Train Accuracy: 0.9940, Validation Accuracy: 0.9824, Loss: 0.0070\n",
            "Epoch   8 Batch   50/538 - Train Accuracy: 0.9863, Validation Accuracy: 0.9830, Loss: 0.0076\n",
            "Epoch   8 Batch   60/538 - Train Accuracy: 0.9850, Validation Accuracy: 0.9842, Loss: 0.0111\n",
            "Epoch   8 Batch   70/538 - Train Accuracy: 0.9829, Validation Accuracy: 0.9751, Loss: 0.0093\n",
            "Epoch   8 Batch   80/538 - Train Accuracy: 0.9914, Validation Accuracy: 0.9748, Loss: 0.0072\n",
            "Epoch   8 Batch   90/538 - Train Accuracy: 0.9779, Validation Accuracy: 0.9854, Loss: 0.0122\n",
            "Epoch   8 Batch  100/538 - Train Accuracy: 0.9967, Validation Accuracy: 0.9824, Loss: 0.0069\n",
            "Epoch   8 Batch  110/538 - Train Accuracy: 0.9951, Validation Accuracy: 0.9783, Loss: 0.0076\n",
            "Epoch   8 Batch  120/538 - Train Accuracy: 0.9914, Validation Accuracy: 0.9830, Loss: 0.0071\n",
            "Epoch   8 Batch  130/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9753, Loss: 0.0121\n",
            "Epoch   8 Batch  140/538 - Train Accuracy: 0.9914, Validation Accuracy: 0.9776, Loss: 0.0128\n",
            "Epoch   8 Batch  150/538 - Train Accuracy: 0.9896, Validation Accuracy: 0.9778, Loss: 0.0099\n",
            "Epoch   8 Batch  160/538 - Train Accuracy: 0.9862, Validation Accuracy: 0.9751, Loss: 0.0085\n",
            "Epoch   8 Batch  170/538 - Train Accuracy: 0.9883, Validation Accuracy: 0.9734, Loss: 0.0097\n",
            "Epoch   8 Batch  180/538 - Train Accuracy: 0.9855, Validation Accuracy: 0.9719, Loss: 0.0094\n",
            "Epoch   8 Batch  190/538 - Train Accuracy: 0.9929, Validation Accuracy: 0.9734, Loss: 0.0097\n",
            "Epoch   8 Batch  200/538 - Train Accuracy: 0.9939, Validation Accuracy: 0.9771, Loss: 0.0072\n",
            "Epoch   8 Batch  210/538 - Train Accuracy: 0.9914, Validation Accuracy: 0.9803, Loss: 0.0091\n",
            "Epoch   8 Batch  220/538 - Train Accuracy: 0.9926, Validation Accuracy: 0.9773, Loss: 0.0073\n",
            "Epoch   8 Batch  230/538 - Train Accuracy: 0.9934, Validation Accuracy: 0.9778, Loss: 0.0104\n",
            "Epoch   8 Batch  240/538 - Train Accuracy: 0.9826, Validation Accuracy: 0.9775, Loss: 0.0119\n",
            "Epoch   8 Batch  250/538 - Train Accuracy: 0.9916, Validation Accuracy: 0.9725, Loss: 0.0095\n",
            "Epoch   8 Batch  260/538 - Train Accuracy: 0.9818, Validation Accuracy: 0.9755, Loss: 0.0153\n",
            "Epoch   8 Batch  270/538 - Train Accuracy: 0.9984, Validation Accuracy: 0.9751, Loss: 0.0078\n",
            "Epoch   8 Batch  280/538 - Train Accuracy: 0.9933, Validation Accuracy: 0.9652, Loss: 0.0101\n",
            "Epoch   8 Batch  290/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9709, Loss: 0.0087\n",
            "Epoch   8 Batch  300/538 - Train Accuracy: 0.9926, Validation Accuracy: 0.9693, Loss: 0.0080\n",
            "Epoch   8 Batch  310/538 - Train Accuracy: 0.9914, Validation Accuracy: 0.9622, Loss: 0.0122\n",
            "Epoch   8 Batch  320/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9688, Loss: 0.0068\n",
            "Epoch   8 Batch  330/538 - Train Accuracy: 0.9939, Validation Accuracy: 0.9700, Loss: 0.0119\n",
            "Epoch   8 Batch  340/538 - Train Accuracy: 0.9895, Validation Accuracy: 0.9744, Loss: 0.0074\n",
            "Epoch   8 Batch  350/538 - Train Accuracy: 0.9857, Validation Accuracy: 0.9721, Loss: 0.0140\n",
            "Epoch   8 Batch  360/538 - Train Accuracy: 0.9891, Validation Accuracy: 0.9782, Loss: 0.0094\n",
            "Epoch   8 Batch  370/538 - Train Accuracy: 0.9879, Validation Accuracy: 0.9691, Loss: 0.0105\n",
            "Epoch   8 Batch  380/538 - Train Accuracy: 0.9986, Validation Accuracy: 0.9785, Loss: 0.0082\n",
            "Epoch   8 Batch  390/538 - Train Accuracy: 0.9855, Validation Accuracy: 0.9711, Loss: 0.0098\n",
            "Epoch   8 Batch  400/538 - Train Accuracy: 0.9903, Validation Accuracy: 0.9796, Loss: 0.0079\n",
            "Epoch   8 Batch  410/538 - Train Accuracy: 0.9971, Validation Accuracy: 0.9730, Loss: 0.0057\n",
            "Epoch   8 Batch  420/538 - Train Accuracy: 0.9836, Validation Accuracy: 0.9723, Loss: 0.0082\n",
            "Epoch   8 Batch  430/538 - Train Accuracy: 0.9891, Validation Accuracy: 0.9689, Loss: 0.0097\n",
            "Epoch   8 Batch  440/538 - Train Accuracy: 0.9852, Validation Accuracy: 0.9782, Loss: 0.0099\n",
            "Epoch   8 Batch  450/538 - Train Accuracy: 0.9827, Validation Accuracy: 0.9705, Loss: 0.0124\n",
            "Epoch   8 Batch  460/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9785, Loss: 0.0103\n",
            "Epoch   8 Batch  470/538 - Train Accuracy: 0.9901, Validation Accuracy: 0.9801, Loss: 0.0063\n",
            "Epoch   8 Batch  480/538 - Train Accuracy: 0.9926, Validation Accuracy: 0.9739, Loss: 0.0091\n",
            "Epoch   8 Batch  490/538 - Train Accuracy: 0.9834, Validation Accuracy: 0.9806, Loss: 0.0122\n",
            "Epoch   8 Batch  500/538 - Train Accuracy: 0.9961, Validation Accuracy: 0.9771, Loss: 0.0064\n",
            "Epoch   8 Batch  510/538 - Train Accuracy: 0.9829, Validation Accuracy: 0.9844, Loss: 0.0084\n",
            "Epoch   8 Batch  520/538 - Train Accuracy: 0.9885, Validation Accuracy: 0.9767, Loss: 0.0094\n",
            "Epoch   8 Batch  530/538 - Train Accuracy: 0.9891, Validation Accuracy: 0.9776, Loss: 0.0085\n",
            "Epoch   9 Batch   10/538 - Train Accuracy: 0.9963, Validation Accuracy: 0.9698, Loss: 0.0087\n",
            "Epoch   9 Batch   20/538 - Train Accuracy: 0.9857, Validation Accuracy: 0.9716, Loss: 0.0138\n",
            "Epoch   9 Batch   30/538 - Train Accuracy: 0.9820, Validation Accuracy: 0.9801, Loss: 0.0101\n",
            "Epoch   9 Batch   40/538 - Train Accuracy: 0.9934, Validation Accuracy: 0.9702, Loss: 0.0060\n",
            "Epoch   9 Batch   50/538 - Train Accuracy: 0.9838, Validation Accuracy: 0.9796, Loss: 0.0089\n",
            "Epoch   9 Batch   60/538 - Train Accuracy: 0.9947, Validation Accuracy: 0.9810, Loss: 0.0114\n",
            "Epoch   9 Batch   70/538 - Train Accuracy: 0.9946, Validation Accuracy: 0.9808, Loss: 0.0053\n",
            "Epoch   9 Batch   80/538 - Train Accuracy: 0.9955, Validation Accuracy: 0.9808, Loss: 0.0058\n",
            "Epoch   9 Batch   90/538 - Train Accuracy: 0.9818, Validation Accuracy: 0.9805, Loss: 0.0098\n",
            "Epoch   9 Batch  100/538 - Train Accuracy: 0.9928, Validation Accuracy: 0.9798, Loss: 0.0072\n",
            "Epoch   9 Batch  110/538 - Train Accuracy: 0.9947, Validation Accuracy: 0.9721, Loss: 0.0072\n",
            "Epoch   9 Batch  120/538 - Train Accuracy: 0.9908, Validation Accuracy: 0.9766, Loss: 0.0059\n",
            "Epoch   9 Batch  130/538 - Train Accuracy: 0.9892, Validation Accuracy: 0.9767, Loss: 0.0080\n",
            "Epoch   9 Batch  140/538 - Train Accuracy: 0.9852, Validation Accuracy: 0.9735, Loss: 0.0120\n",
            "Epoch   9 Batch  150/538 - Train Accuracy: 0.9887, Validation Accuracy: 0.9783, Loss: 0.0095\n",
            "Epoch   9 Batch  160/538 - Train Accuracy: 0.9933, Validation Accuracy: 0.9757, Loss: 0.0074\n",
            "Epoch   9 Batch  170/538 - Train Accuracy: 0.9874, Validation Accuracy: 0.9766, Loss: 0.0086\n",
            "Epoch   9 Batch  180/538 - Train Accuracy: 0.9859, Validation Accuracy: 0.9734, Loss: 0.0090\n",
            "Epoch   9 Batch  190/538 - Train Accuracy: 0.9927, Validation Accuracy: 0.9846, Loss: 0.0090\n",
            "Epoch   9 Batch  200/538 - Train Accuracy: 0.9947, Validation Accuracy: 0.9815, Loss: 0.0079\n",
            "Epoch   9 Batch  210/538 - Train Accuracy: 0.9933, Validation Accuracy: 0.9819, Loss: 0.0081\n",
            "Epoch   9 Batch  220/538 - Train Accuracy: 0.9898, Validation Accuracy: 0.9751, Loss: 0.0095\n",
            "Epoch   9 Batch  230/538 - Train Accuracy: 0.9936, Validation Accuracy: 0.9718, Loss: 0.0111\n",
            "Epoch   9 Batch  240/538 - Train Accuracy: 0.9861, Validation Accuracy: 0.9703, Loss: 0.0085\n",
            "Epoch   9 Batch  250/538 - Train Accuracy: 0.9967, Validation Accuracy: 0.9730, Loss: 0.0085\n",
            "Epoch   9 Batch  260/538 - Train Accuracy: 0.9849, Validation Accuracy: 0.9718, Loss: 0.0116\n",
            "Epoch   9 Batch  270/538 - Train Accuracy: 0.9887, Validation Accuracy: 0.9705, Loss: 0.0078\n",
            "Epoch   9 Batch  280/538 - Train Accuracy: 0.9911, Validation Accuracy: 0.9714, Loss: 0.0077\n",
            "Epoch   9 Batch  290/538 - Train Accuracy: 0.9936, Validation Accuracy: 0.9732, Loss: 0.0101\n",
            "Epoch   9 Batch  300/538 - Train Accuracy: 0.9942, Validation Accuracy: 0.9743, Loss: 0.0072\n",
            "Epoch   9 Batch  310/538 - Train Accuracy: 0.9918, Validation Accuracy: 0.9744, Loss: 0.0085\n",
            "Epoch   9 Batch  320/538 - Train Accuracy: 0.9851, Validation Accuracy: 0.9796, Loss: 0.0108\n",
            "Epoch   9 Batch  330/538 - Train Accuracy: 0.9929, Validation Accuracy: 0.9801, Loss: 0.0066\n",
            "Epoch   9 Batch  340/538 - Train Accuracy: 0.9910, Validation Accuracy: 0.9773, Loss: 0.0063\n",
            "Epoch   9 Batch  350/538 - Train Accuracy: 0.9890, Validation Accuracy: 0.9709, Loss: 0.0109\n",
            "Epoch   9 Batch  360/538 - Train Accuracy: 0.9930, Validation Accuracy: 0.9764, Loss: 0.0073\n",
            "Epoch   9 Batch  370/538 - Train Accuracy: 0.9799, Validation Accuracy: 0.9787, Loss: 0.0095\n",
            "Epoch   9 Batch  380/538 - Train Accuracy: 0.9941, Validation Accuracy: 0.9744, Loss: 0.0064\n",
            "Epoch   9 Batch  390/538 - Train Accuracy: 0.9859, Validation Accuracy: 0.9735, Loss: 0.0072\n",
            "Epoch   9 Batch  400/538 - Train Accuracy: 0.9929, Validation Accuracy: 0.9759, Loss: 0.0078\n",
            "Epoch   9 Batch  410/538 - Train Accuracy: 0.9932, Validation Accuracy: 0.9753, Loss: 0.0077\n",
            "Epoch   9 Batch  420/538 - Train Accuracy: 0.9816, Validation Accuracy: 0.9730, Loss: 0.0096\n",
            "Epoch   9 Batch  430/538 - Train Accuracy: 0.9871, Validation Accuracy: 0.9766, Loss: 0.0085\n",
            "Epoch   9 Batch  440/538 - Train Accuracy: 0.9959, Validation Accuracy: 0.9656, Loss: 0.0084\n",
            "Epoch   9 Batch  450/538 - Train Accuracy: 0.9892, Validation Accuracy: 0.9737, Loss: 0.0102\n",
            "Epoch   9 Batch  460/538 - Train Accuracy: 0.9892, Validation Accuracy: 0.9790, Loss: 0.0092\n",
            "Epoch   9 Batch  470/538 - Train Accuracy: 0.9875, Validation Accuracy: 0.9831, Loss: 0.0072\n",
            "Epoch   9 Batch  480/538 - Train Accuracy: 0.9942, Validation Accuracy: 0.9821, Loss: 0.0082\n",
            "Epoch   9 Batch  490/538 - Train Accuracy: 0.9950, Validation Accuracy: 0.9847, Loss: 0.0081\n",
            "Epoch   9 Batch  500/538 - Train Accuracy: 0.9954, Validation Accuracy: 0.9830, Loss: 0.0084\n",
            "Epoch   9 Batch  510/538 - Train Accuracy: 0.9926, Validation Accuracy: 0.9746, Loss: 0.0068\n",
            "Epoch   9 Batch  520/538 - Train Accuracy: 0.9947, Validation Accuracy: 0.9771, Loss: 0.0094\n",
            "Epoch   9 Batch  530/538 - Train Accuracy: 0.9971, Validation Accuracy: 0.9759, Loss: 0.0065\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-BKLvGb-_VyX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_params(save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qso2svjJ_Vyq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
        "load_path = load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1gPsPY-6dw96",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}